| Model             | Model Name            | Parameters  | Architecture       | Architecture Type | Main Use Case                                      | Open-source      |
|-------------------|------------------------|-------------|--------------------|-------------------|---------------------------------------------------|------------------|
| **GPT-2 Small**   | `gpt2`                 | 117M        | Transformer        | Decoder           | Text generation, simpler tasks                    | Yes              |
| **GPT-2 Medium**  | `gpt2-medium`          | 345M        | Transformer        | Decoder           | Text generation, dialogue                         | Yes              |
| **BERT Base**     | `bert-base-uncased`      | 110M        | Transformer        | Encoder           | Text classification, token classification, Q&A    | Yes              |
| **BERT Large**    | `bert-large-uncased`     | 340M        | Transformer        | Encoder           | More complex NLP tasks, Q&A, language modeling     | Yes              |
| **T5 Small**      | `t5-small`              | 60M         | Text-to-Text Transformer | Encoder-Decoder   | Text generation, translation, summarization        | Yes              |
| **T5 Base**       | `t5-base`               | 220M        | Text-to-Text Transformer | Encoder-Decoder   | Text-to-text tasks, summarization, Q&A             | Yes              |
| **FLAN-T5 Small** | `google/flan-t5-small`   | 80M         | Text-to-Text Transformer | Encoder-Decoder   | Instruction-based text tasks                       | Yes              |
| **FLAN-T5 Base**  | `google/flan-t5-base`    | 250M        | Text-to-Text Transformer | Encoder-Decoder   | Instruction-based text tasks, summarization        | Yes              |
| **FLAN-T5 Large** | `google/flan-t5-large`   | 780M        | Text-to-Text Transformer | Encoder-Decoder   | Text-to-text generation, complex tasks             | Yes              |
| **RoBERTa Base**  | `roberta-base`         | 125M        | Transformer        | Encoder           | Text classification, token classification, Q&A    | Yes              |
| **RoBERTa Large** | `roberta-large`        | 355M        | Transformer        | Encoder           | More complex NLP tasks, Q&A, language modeling     | Yes              |
| **BART Base**     | `facebook/bart-base`   | 140M        | Transformer        | Encoder-Decoder   | Text generation, summarization, translation        | Yes              |
| **BART Large**    | `facebook/bart-large`  | 400M        | Transformer        | Encoder-Decoder   | High-quality text generation, summarization       | Yes              |
| **LLaMA 7B**      | `llama-7b`             | 7B          | Transformer        | Decoder           | General-purpose text generation, language modeling | Yes              |
