## GPT models

| Model             | Model Name            | Parameters  | Architecture       | Architecture Type | Main Use Case                                      | Open-source      |
|-------------------|------------------------|-------------|--------------------|-------------------|---------------------------------------------------|------------------|
| **GPT-1**         | `gpt`                  | 117M        | Transformer        | Decoder           | Text generation, language modeling                | No               |
| **GPT-2 Small**   | `gpt2`                 | 117M        | Transformer        | Decoder           | Text generation, simpler tasks                    | Yes              |
| **GPT-2 Medium**  | `gpt2-medium`          | 345M        | Transformer        | Decoder           | Text generation, dialogue                         | Yes              |
| **GPT-2 Large**   | `gpt2-large`           | 774M        | Transformer        | Decoder           | Long-form text, story generation                  | Yes              |
| **GPT-2 XL**      | `gpt2-xl`              | 1.5B        | Transformer        | Decoder           | Advanced text generation                          | Yes              |
| **GPT-3 Ada**     | `ada`                  | 350M        | Transformer        | Decoder           | Simple text generation                            | No               |
| **GPT-3 Babbage** | `babbage`              | 1.3B        | Transformer        | Decoder           | Intermediate text generation                      | No               |
| **GPT-3 Curie**   | `curie`                | 6.7B        | Transformer        | Decoder           | More complex tasks, chatbots                      | No               |
| **GPT-3 Davinci** | `davinci`              | 175B        | Transformer        | Decoder           | High-quality text, complex tasks                  | No               |
| **GPT-4**         | `gpt-4`                | Unknown     | Transformer        | Decoder           | Advanced reasoning, multi-modal                   | No               |
| **Codex**         | `codex`                | 12B, 175B   | Transformer        | Decoder           | Code generation, debugging                        | No               |


## Google Models

| Model             | Model Name              | Parameters  | Architecture       | Architecture Type | Main Use Case                                      | Open-source      |
|-------------------|--------------------------|-------------|--------------------|-------------------|---------------------------------------------------|------------------|
| **BERT Base**     | `bert-base-uncased`      | 110M        | Transformer        | Encoder           | Text classification, token classification, Q&A    | Yes              |
| **BERT Large**    | `bert-large-uncased`     | 340M        | Transformer        | Encoder           | More complex NLP tasks, Q&A, language modeling     | Yes              |
| **T5 Small**      | `t5-small`              | 60M         | Text-to-Text Transformer | Encoder-Decoder   | Text generation, translation, summarization        | Yes              |
| **T5 Base**       | `t5-base`               | 220M        | Text-to-Text Transformer | Encoder-Decoder   | Text-to-text tasks, summarization, Q&A             | Yes              |
| **T5 Large**      | `t5-large`              | 770M        | Text-to-Text Transformer | Encoder-Decoder   | High-quality text generation, complex NLP tasks    | Yes              |
| **T5 XL**         | `t5-3b`                 | 3B          | Text-to-Text Transformer | Encoder-Decoder   | Large-scale text-to-text tasks, multi-task learning| Yes              |
| **T5 XXL**        | `t5-11b`                | 11B         | Text-to-Text Transformer | Encoder-Decoder   | Advanced text generation, summarization, reasoning | Yes              |
| **PaLM 2-Small**  | `palm-2-small`          | Unknown     | Transformer        | Decoder           | Text generation, reasoning, code completion        | No               |
| **PaLM 2-Medium** | `palm-2-medium`         | Unknown     | Transformer        | Decoder           | Multi-task learning, complex text generation       | No               |
| **PaLM 2-Large**  | `palm-2-large`          | Unknown     | Transformer        | Decoder           | High-quality text generation, advanced reasoning   | No               |
| **PaLM 2-XL**     | `palm-2-xl`             | Unknown     | Transformer        | Decoder           | Cutting-edge NLP tasks, multi-modal applications   | No               |
| **FLAN-T5 Small** | `google/flan-t5-small`   | 80M         | Text-to-Text Transformer | Encoder-Decoder   | Instruction-based text tasks                       | Yes              |
| **FLAN-T5 Base**  | `google/flan-t5-base`    | 250M        | Text-to-Text Transformer | Encoder-Decoder   | Instruction-based text tasks, summarization        | Yes              |
| **FLAN-T5 Large** | `google/flan-t5-large`   | 780M        | Text-to-Text Transformer | Encoder-Decoder   | Text-to-text generation, complex tasks             | Yes              |
| **FLAN-T5 XL**    | `google/flan-t5-xl`      | 3B          | Text-to-Text Transformer | Encoder-Decoder   | Large-scale text-to-text generation                | Yes              |
| **FLAN-T5 XXL**   | `google/flan-t5-xxl`     | 11B         | Text-to-Text Transformer | Encoder-Decoder   | Advanced NLP, instruction following, reasoning     | Yes              |


## facebook models

| Model             | Model Name            | Parameters  | Architecture       | Architecture Type | Main Use Case                                      | Open-source      |
|-------------------|------------------------|-------------|--------------------|-------------------|---------------------------------------------------|------------------|
| **RoBERTa Base**  | `roberta-base`         | 125M        | Transformer        | Encoder           | Text classification, token classification, Q&A    | Yes              |
| **RoBERTa Large** | `roberta-large`        | 355M        | Transformer        | Encoder           | More complex NLP tasks, Q&A, language modeling     | Yes              |
| **BART Base**     | `facebook/bart-base`   | 140M        | Transformer        | Encoder-Decoder   | Text generation, summarization, translation        | Yes              |
| **BART Large**    | `facebook/bart-large`  | 400M        | Transformer        | Encoder-Decoder   | High-quality text generation, summarization       | Yes              |
| **LLaMA 7B**      | `llama-7b`             | 7B          | Transformer        | Decoder           | General-purpose text generation, language modeling | Yes              |
| **LLaMA 13B**     | `llama-13b`            | 13B         | Transformer        | Decoder           | More complex text generation, advanced NLP tasks   | Yes              |
| **LLaMA 30B**     | `llama-30b`            | 30B         | Transformer        | Decoder           | High-capacity text generation, large-scale tasks   | Yes              |
| **LLaMA 65B**     | `llama-65b`            | 65B         | Transformer        | Decoder           | Cutting-edge text generation, extensive language understanding | Yes  |